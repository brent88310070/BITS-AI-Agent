import psutil
import GPUtil
import pynvml
from datetime import datetime
from langchain.tools import tool
from langchain_ollama import ChatOllama

# ============================================================
# ğŸ§© å·¥å…·å®šç¾©
# ============================================================

@tool("system_info")
def system_info() -> str:
    """CPU and RAM usage"""
    mem = psutil.virtual_memory()
    return (f"ğŸ–¥ï¸ CPU: {psutil.cpu_count()} cores, Usage: {psutil.cpu_percent()}%\n"
            f"ğŸ’¾ RAM: {mem.used / 1e9:.2f}/{mem.total / 1e9:.2f} GB")

@tool("get_time")
def get_time() -> str:
    """Current time"""
    time = datetime.now().strftime("ğŸ•’ %Y-%m-%d (%A) %H:%M:%S")
    print(time)
    return time

@tool("gpu_info")
def gpu_info() -> str:
    """Get NVIDIA GPU status (Load, Memory, Temperature)"""
    try:
        gpus = GPUtil.getGPUs()
        if not gpus:
            return "âŒ No NVIDIA GPU detected."
        
        info = []
        for gpu in gpus:
            used_gb = gpu.memoryUsed / 1024
            total_gb = gpu.memoryTotal / 1024
            gpu_status = (f"ğŸ® GPU: {gpu.name} | Load: {gpu.load*100:.1f}% | "
                          f"Temp: {gpu.temperature}Â°C | "
                          f"Mem: {used_gb:.2f}/{total_gb:.2f} GB")
            info.append(gpu_status)
        return "\n".join(info)
    except Exception as e:
        return f"âš ï¸ Could not retrieve GPU info: {str(e)}"

@tool("disk_info")
def disk_info() -> str:
    """Get Disk/Storage usage for the root directory"""
    usage = psutil.disk_usage('/')
    return (f"ğŸ’½ Disk: {usage.used / 1e9:.2f}/{usage.total / 1e9:.2f} GB "
            f"({usage.percent}% used)")

@tool("resource_monitor")
def resource_monitor() -> str:
    """Identify top CPU, RAM, and GPU consuming processes and their scripts."""
    processes = []
    
    # 1. å–å¾— CPU èˆ‡ RAM çš„è¡Œç¨‹è³‡è¨Š
    for proc in psutil.process_iter(['pid', 'name', 'username', 'cpu_percent', 'memory_info', 'cmdline']):
        try:
            info = proc.info
            cmdline = " ".join(info['cmdline']) if info['cmdline'] else ""
            is_script = any(ext in cmdline.lower() for ext in ['.py', '.sh', '.r', '.pl', '.ipynb', 'python', 'node'])
            
            processes.append({
                'pid': info['pid'],
                'user': info['username'],
                'name': info['name'],
                'cpu': info['cpu_percent'],
                'mem': info['memory_info'].rss / 1e9,
                'script': cmdline if is_script else None,
                'gpu_mem': 0, # é è¨­ç‚º 0
                'gpu_id': None
            })
        except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):
            pass

    # 2. å–å¾— GPU è¡Œç¨‹è³‡è¨Š (ä½¿ç”¨ NVML)
    gpu_processes = []
    try:
        pynvml.nvmlInit()
        device_count = pynvml.nvmlDeviceGetCount()
        for i in range(device_count):
            handle = pynvml.nvmlDeviceGetHandleByIndex(i)
            procs = pynvml.nvmlDeviceGetComputeRunningProcesses(handle)
            for p in procs:
                gpu_processes.append({
                    'pid': p.pid,
                    'gpu_id': i,
                    'gpu_mem': p.usedGpuMemory / 1e9 # è½‰ç‚º GB
                })
        pynvml.nvmlShutdown()
    except Exception:
        pass # è‹¥ç„¡ GPU å‰‡è·³é

    # 3. å°‡ GPU æ•¸æ“šåˆä½µå›ä¸»åˆ—è¡¨
    for gp in gpu_processes:
        for p in processes:
            if p['pid'] == gp['pid']:
                p['gpu_mem'] = gp['gpu_mem']
                p['gpu_id'] = gp['gpu_id']

    # 4. æ’åºä¸¦ç”¢å‡ºçµæœ
    top_cpu = sorted(processes, key=lambda x: x['cpu'], reverse=True)[:3]
    top_mem = sorted(processes, key=lambda x: x['mem'], reverse=True)[:3]
    top_gpu = sorted([p for p in processes if p['gpu_mem'] > 0], key=lambda x: x['gpu_mem'], reverse=True)[:3]

    result = ["ğŸ“Š **Resource Consumption Leaderboard**"]
    
    result.append("\nğŸ”¥ Top 3 CPU Usage:")
    for p in top_cpu:
        s = f" (ğŸ“œ {p['script'][:40]}...)" if p['script'] else ""
        result.append(f"- User: {p['user']} | CPU: {p['cpu']}% | Proc: {p['name']}{s}")

    result.append("\nğŸ§  Top 3 RAM Usage:")
    for p in top_mem:
        s = f" (ğŸ“œ {p['script'][:40]}...)" if p['script'] else ""
        result.append(f"- User: {p['user']} | Mem: {p['mem']:.2f} GB | Proc: {p['name']}{s}")

    if top_gpu:
        result.append("\nğŸ® Top 3 GPU Usage:")
        for p in top_gpu:
            s = f" (ğŸ“œ {p['script'][:40]}...)" if p['script'] else ""
            result.append(f"- User: {p['user']} | GPU[{p['gpu_id']}] VRAM: {p['gpu_mem']:.2f} GB | Proc: {p['name']}{s}")
    else:
        result.append("\nğŸ® GPU Usage: No active GPU compute processes found.")

    return "\n".join(result)

# ============================================================
# âš™ï¸ å·¥å…·åˆ—è¡¨èˆ‡ Agent å»ºç«‹
# ============================================================

# å°‡æ–°å¢çš„å·¥å…·åŠ å…¥æ¸…å–®
TOOLS = [system_info, get_time, gpu_info, disk_info, resource_monitor]

def create_tool_agent(llm_name: str, tools: list):
    """å»ºç«‹ä¸¦å›å‚³ä¸€å€‹å·²ç¶å®šå·¥å…·çš„ ChatOllama Agent"""
    # temperature è¨­ç‚º 0.1 ä»¥æé«˜å·¥å…·é¸æ“‡çš„ç©©å®šæ€§
    agent_tools = ChatOllama(model=llm_name, temperature=0.1).bind_tools(tools)
    return agent_tools

# æ¸¬è©¦ Agent
# agent = create_tool_agent("llama3.1", TOOLS)